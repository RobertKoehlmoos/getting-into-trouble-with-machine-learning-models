{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "These are the only imports you should need for the whole notebook"
      ],
      "metadata": {
        "id": "axjKHs_Rak6o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjVNxO8rH4YW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading our dataset\n",
        "Loading in the MNIST dataset. Don't worry too much about what's going on here, where just setting up the framework for pulling images and their labels from the training and testing sets."
      ],
      "metadata": {
        "id": "35KKTx98aIMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5), (0.5))])\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n"
      ],
      "metadata": {
        "id": "3M0VLMe0Je2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at what we're dealing with"
      ],
      "metadata": {
        "id": "3_26vg2sahH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# functions to show an image\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# print labels\n",
        "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
      ],
      "metadata": {
        "id": "ixd1RMrlLRA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Neural Network\n",
        "\n",
        "Let's define a structure for a single layer neural network. This is what the deep learning experts spend a lot of time tinkering with to figure out the best structures fora problem. This is pretty much the simplest network possible."
      ],
      "metadata": {
        "id": "VAuLebT0au8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the class for single layer NN\n",
        "class one_layer_net(torch.nn.Module):\n",
        "    # Constructor\n",
        "    def __init__(self, input_size, hidden_neurons, output_size):\n",
        "        super(one_layer_net, self).__init__()\n",
        "        # hidden layer\n",
        "        self.linear_one = torch.nn.Linear(input_size, hidden_neurons)\n",
        "        self.linear_two = torch.nn.Linear(hidden_neurons, output_size)\n",
        "        # defining layers as attributes\n",
        "        self.layer_in = None\n",
        "        self.act = None\n",
        "        self.layer_out = None\n",
        "    # prediction function\n",
        "    def forward(self, x):\n",
        "        flattened_input = torch.flatten(x, 1)\n",
        "        self.layer_in = self.linear_one(x)\n",
        "        self.act = torch.sigmoid(self.layer_in)\n",
        "        self.layer_out = self.linear_two(self.act)\n",
        "        y_pred = torch.sigmoid(self.linear_two(self.act))\n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "70wbKrfVNQG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's make one for MNIST\n",
        "Our input is 784 in size because that's how many pixel values we'll get  \n",
        "The output size is 10 because that's how many classes we have to predict, one for each digit  \n",
        "The hidden layer size is 42 as a fun starting number"
      ],
      "metadata": {
        "id": "9Fd0harVbEA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = one_layer_net(28*28, 42, 10)"
      ],
      "metadata": {
        "id": "9GHPChhtOfkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need a way of updating the weights in our network based on loss, known as an optimizer, so we choose a method provided by pytorch. Don't worry too much about the specifics of this part, we're using Stochastic Gradient Descent (SGD), which is pretty standard."
      ],
      "metadata": {
        "id": "gdi4CmABbhmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "3KAKrd51PbUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's finally time to train! We'll start with two passes over the data and update the model weights after every batch. We previosuly defined a batch as being 4 images in size."
      ],
      "metadata": {
        "id": "3kX658pib8TE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(2):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "id": "Ln25JnVGPoUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the model is trained we can save it's weights to a file. We'll use this later, but this is also what can be shared so others can use the model you built."
      ],
      "metadata": {
        "id": "OTM1x_bvcz9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = './mnist_model_weights.pth'\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "-8rEvT6lQqtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing\n",
        "Let's test to see how our model does.  \n",
        "When evaluating models it's important to have a seperate test set of inputs that the model has not seen before, otherwise a model can potentially memorize specifics from it's training set that do not apply beyond, this is known as overfitting, and avoiding it is another big piece of machine learning."
      ],
      "metadata": {
        "id": "MbEeB7tUcN5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataiter = iter(testloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# print images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))"
      ],
      "metadata": {
        "id": "ZKBOdJDrXdl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = one_layer_net(28*28, 42, 10)\n",
        "net.load_state_dict(torch.load(PATH))"
      ],
      "metadata": {
        "id": "9HM7jb3OWzM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll make some example predicitons."
      ],
      "metadata": {
        "id": "0BRVfDVScxW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_, predicted = torch.max(outputs, 1)\n",
        "\n",
        "print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}'\n",
        "                              for j in range(4)))"
      ],
      "metadata": {
        "id": "B-Y0gQyIXgET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, run the model against the full test dataset and see how it did overall and per class!"
      ],
      "metadata": {
        "id": "PtKv1s7ydQJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare to count predictions for each class\n",
        "correct_pred = {classname: 0 for classname in classes}\n",
        "total_pred = {classname: 0 for classname in classes}\n",
        "\n",
        "# again no gradients needed\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        # collect the correct predictions for each class\n",
        "        for label, prediction in zip(labels, predictions):\n",
        "            if label == prediction:\n",
        "                correct_pred[classes[label]] += 1\n",
        "            total_pred[classes[label]] += 1\n",
        "\n",
        "\n",
        "# print accuracy\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * sum(correct_pred.values()) // 10000} %')\n",
        "for classname, correct_count in correct_pred.items():\n",
        "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
      ],
      "metadata": {
        "id": "zQtt98egXovs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xpi6OqnVYIk0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}